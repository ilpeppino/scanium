# Grafana Alerting - Alert Rules Provisioning
# Baseline alerts for Scanium mobile app monitoring
#
# Alerts defined:
# A) Error rate spike (Loki) - Detects unusual error log volume
# B) Telemetry drop (Loki) - Detects when no telemetry is received
# C) Inference latency regression (Mimir) - Detects ML inference slowdowns
# D) Crash spike (Sentry) - Not implemented here; see README for Sentry setup

apiVersion: 1

groups:
  # ============================================================================
  # Error Rate Alerts (Loki-based)
  # ============================================================================
  - orgId: 1
    name: Scanium - Error Rate
    folder: Scanium Alerts
    interval: 1m
    rules:
      # A) Error Rate Spike - Production
      - uid: error-rate-spike-prod
        title: "Error Rate Spike (prod)"
        condition: C
        data:
          # Query A: Count error logs over 10m
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: LOKI
            model:
              expr: |
                sum(count_over_time({source="scanium-mobile", env="prod"} |~ `(?i)error|exception|fatal|crash` [10m]))
              queryType: instant
              refId: A
          # Query B: Reduce to single value
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          # Query C: Threshold check (> 50 errors in 10m = alert)
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 50  # Threshold: > 50 errors/10min
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          summary: "High error rate detected in production"
          description: "Error count exceeded 50 in the last 10 minutes. Check logs for details."
          runbook_url: "https://github.com/ilpeppino/scanium/wiki/runbooks/error-rate-spike"
        labels:
          severity: critical
          env: prod
          platform: android

      # A) Error Rate Spike - Staging
      - uid: error-rate-spike-stage
        title: "Error Rate Spike (stage)"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: LOKI
            model:
              expr: |
                sum(count_over_time({source="scanium-mobile", env="stage"} |~ `(?i)error|exception|fatal|crash` [10m]))
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 100  # Higher threshold for staging
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 10m
        annotations:
          summary: "High error rate detected in staging"
          description: "Error count exceeded 100 in the last 10 minutes."
        labels:
          severity: warning
          env: stage
          platform: android

      # A) Error Rate Spike - Dev (informational only)
      - uid: error-rate-spike-dev
        title: "Error Rate Spike (dev)"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: LOKI
            model:
              expr: |
                sum(count_over_time({source="scanium-mobile", env="dev"} |~ `(?i)error|exception|fatal|crash` [10m]))
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 200  # Higher threshold for dev
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 15m
        annotations:
          summary: "High error rate detected in dev"
          description: "Error count exceeded 200 in the last 10 minutes."
        labels:
          severity: info
          env: dev
          platform: android

  # ============================================================================
  # Telemetry Health Alerts (Loki-based)
  # ============================================================================
  - orgId: 1
    name: Scanium - Telemetry Health
    folder: Scanium Alerts
    interval: 5m
    rules:
      # B) Telemetry Drop - No events received (prod only)
      - uid: telemetry-drop-prod
        title: "Telemetry Drop (prod)"
        condition: C
        data:
          # Query A: Count ANY logs in last 15m
          - refId: A
            relativeTimeRange:
              from: 900
              to: 0
            datasourceUid: LOKI
            model:
              expr: |
                sum(count_over_time({source="scanium-mobile", env="prod"} [15m])) or vector(0)
              queryType: instant
              refId: A
          # Query B: Reduce
          - refId: B
            relativeTimeRange:
              from: 900
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          # Query C: Alert if count == 0
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 1  # Alert if < 1 event (i.e., 0 events)
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: Alerting  # Important: No data = telemetry problem!
        execErrState: Error
        for: 15m
        annotations:
          summary: "No telemetry received from production"
          description: "No scan events or logs received in the last 15 minutes. Check app connectivity and Alloy receiver."
          runbook_url: "https://github.com/ilpeppino/scanium/wiki/runbooks/telemetry-drop"
        labels:
          severity: critical
          env: prod
          type: availability

      # B) Telemetry Drop - Check for session_started events specifically
      - uid: no-sessions-prod
        title: "No Scan Sessions (prod)"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 1800  # 30 minutes
              to: 0
            datasourceUid: LOKI
            model:
              expr: |
                sum(count_over_time({source="scanium-mobile", env="prod"} |= `scan.session_started` [30m])) or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 1800
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 1
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: Alerting
        execErrState: Error
        for: 30m
        annotations:
          summary: "No scan sessions started in production"
          description: "No scan.session_started events in the last 30 minutes. This could indicate app issues or zero usage."
        labels:
          severity: warning
          env: prod
          type: usage

  # ============================================================================
  # Performance Alerts (Mimir/Prometheus-based)
  # ============================================================================
  - orgId: 1
    name: Scanium - Performance
    folder: Scanium Alerts
    interval: 1m
    rules:
      # C) Inference Latency Regression - Production
      - uid: inference-latency-prod
        title: "Inference Latency Regression (prod)"
        condition: C
        data:
          # Query A: P95 latency over 15m
          # Note: Assumes histogram metric 'ml_inference_latency_ms_bucket' or similar
          # Adjust metric name based on actual telemetry
          - refId: A
            relativeTimeRange:
              from: 900
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                histogram_quantile(0.95,
                  sum by (le) (
                    rate(ml_inference_latency_ms_bucket{env="prod"}[15m])
                  )
                )
              queryType: instant
              refId: A
          # Query B: Reduce
          - refId: B
            relativeTimeRange:
              from: 900
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          # Query C: Alert if p95 > 2000ms (2 seconds)
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 2000  # Threshold: p95 > 2000ms
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 10m
        annotations:
          summary: "ML inference latency regression in production"
          description: "P95 inference latency exceeded 2000ms over the last 15 minutes. Model performance may be degraded."
          runbook_url: "https://github.com/ilpeppino/scanium/wiki/runbooks/latency-regression"
        labels:
          severity: warning
          env: prod
          component: ml

      # C) Inference Latency Regression - Staging (higher threshold)
      - uid: inference-latency-stage
        title: "Inference Latency Regression (stage)"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 900
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                histogram_quantile(0.95,
                  sum by (le) (
                    rate(ml_inference_latency_ms_bucket{env="stage"}[15m])
                  )
                )
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 900
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 3000  # Higher threshold for staging: 3s
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 15m
        annotations:
          summary: "ML inference latency regression in staging"
          description: "P95 inference latency exceeded 3000ms over the last 15 minutes."
        labels:
          severity: info
          env: stage
          component: ml

  # ============================================================================
  # Pipeline Health Alerts (Self-Observability)
  # ============================================================================
  # Monitors the health of the observability pipeline itself (Alloy, Loki, Tempo, Mimir)
  - orgId: 1
    name: Scanium - Pipeline Health
    folder: Scanium Alerts
    interval: 1m
    rules:
      # Service Down - Alloy
      - uid: pipeline-alloy-down
        title: "Alloy Down"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                up{job="alloy", source="pipeline"} or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 1  # up < 1 means down
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: Alerting
        execErrState: Error
        for: 2m
        annotations:
          summary: "Alloy collector is down"
          description: "Alloy OTLP receiver is not responding. No telemetry can be ingested."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#troubleshooting"
        labels:
          severity: critical
          component: alloy
          type: availability

      # Service Down - Loki
      - uid: pipeline-loki-down
        title: "Loki Down"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                up{job="loki", source="pipeline"} or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 1
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: Alerting
        execErrState: Error
        for: 2m
        annotations:
          summary: "Loki is down"
          description: "Loki log storage is not responding. Logs cannot be ingested or queried."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#troubleshooting"
        labels:
          severity: critical
          component: loki
          type: availability

      # Service Down - Tempo
      - uid: pipeline-tempo-down
        title: "Tempo Down"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                up{job="tempo", source="pipeline"} or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 1
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: Alerting
        execErrState: Error
        for: 2m
        annotations:
          summary: "Tempo is down"
          description: "Tempo trace storage is not responding. Traces cannot be ingested or queried."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#troubleshooting"
        labels:
          severity: critical
          component: tempo
          type: availability

      # Service Down - Mimir
      - uid: pipeline-mimir-down
        title: "Mimir Down"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                up{job="mimir", source="pipeline"} or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 1
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: Alerting
        execErrState: Error
        for: 2m
        annotations:
          summary: "Mimir is down"
          description: "Mimir metrics storage is not responding. Metrics cannot be ingested or queried."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#troubleshooting"
        labels:
          severity: critical
          component: mimir
          type: availability

      # Alloy Exporter Failures (logs)
      - uid: pipeline-exporter-failed-logs
        title: "Alloy Log Export Failures"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                sum(increase(otelcol_exporter_send_failed_log_records{source="pipeline"}[5m])) or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0  # Any failures trigger alert
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          summary: "Alloy failing to export logs"
          description: "Alloy has failed to export log records to Loki in the last 5 minutes. Check Loki health and connectivity."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#pipeline-troubleshooting"
        labels:
          severity: critical
          component: alloy
          signal: logs
          type: exporter

      # Alloy Exporter Failures (metrics)
      - uid: pipeline-exporter-failed-metrics
        title: "Alloy Metric Export Failures"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                sum(increase(otelcol_exporter_send_failed_metric_points{source="pipeline"}[5m])) or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          summary: "Alloy failing to export metrics"
          description: "Alloy has failed to export metric points to Mimir in the last 5 minutes. Check Mimir health and connectivity."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#pipeline-troubleshooting"
        labels:
          severity: critical
          component: alloy
          signal: metrics
          type: exporter

      # Alloy Exporter Failures (traces)
      - uid: pipeline-exporter-failed-spans
        title: "Alloy Span Export Failures"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                sum(increase(otelcol_exporter_send_failed_spans{source="pipeline"}[5m])) or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          summary: "Alloy failing to export traces"
          description: "Alloy has failed to export spans to Tempo in the last 5 minutes. Check Tempo health and connectivity."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#pipeline-troubleshooting"
        labels:
          severity: critical
          component: alloy
          signal: traces
          type: exporter

      # Alloy Receiver Refusing Data (indicates overload or errors)
      - uid: pipeline-receiver-refused
        title: "Alloy Receiver Refusing Data"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                sum(increase(otelcol_receiver_refused_log_records{source="pipeline"}[5m]))
                + sum(increase(otelcol_receiver_refused_metric_points{source="pipeline"}[5m]))
                + sum(increase(otelcol_receiver_refused_spans{source="pipeline"}[5m]))
                or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          summary: "Alloy OTLP receiver refusing data"
          description: "Alloy OTLP receiver is refusing incoming telemetry. This could indicate backpressure, malformed data, or resource constraints."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#pipeline-troubleshooting"
        labels:
          severity: warning
          component: alloy
          type: receiver

      # Queue Backpressure Alert
      - uid: pipeline-queue-backpressure
        title: "Alloy Queue Backpressure"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                max(
                  otelcol_exporter_queue_size{source="pipeline"} /
                  otelcol_exporter_queue_capacity{source="pipeline"}
                ) * 100 or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 80  # Alert at 80% queue capacity
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          summary: "Alloy exporter queue is filling up"
          description: "One or more Alloy exporter queues are above 80% capacity. This indicates backpressure and potential data loss if it continues."
          runbook_url: "https://github.com/ilpeppino/scanium/blob/main/monitoring/README.md#pipeline-troubleshooting"
        labels:
          severity: warning
          component: alloy
          type: backpressure

  # ============================================================================
  # Backend API Alerts (HTTP-based)
  # ============================================================================
  # Monitors backend API health using OpenTelemetry HTTP semantic conventions
  - orgId: 1
    name: Scanium - Backend API
    folder: Scanium Alerts
    interval: 1m
    rules:
      # Backend 5xx Error Rate Spike - Production
      - uid: backend-5xx-error-rate-prod
        title: "Backend 5xx Error Rate Spike (prod)"
        condition: C
        data:
          # Query A: Calculate 5xx error rate percentage
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                sum(rate(http_server_request_duration_seconds_count{deployment_environment="prod", http_response_status_code=~"5.."}[5m]))
                / sum(rate(http_server_request_duration_seconds_count{deployment_environment="prod"}[5m]))
                * 100 or vector(0)
              queryType: instant
              refId: A
          # Query B: Reduce to single value
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          # Query C: Threshold check (> 5% error rate = alert)
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 5  # Threshold: > 5% 5xx error rate
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          summary: "High 5xx error rate in production backend"
          description: "Backend API 5xx error rate exceeded 5% over the last 5 minutes. Check backend logs and traces for root cause."
          runbook_url: "https://github.com/ilpeppino/scanium/wiki/runbooks/backend-5xx-spike"
          dashboard_url: "/d/scanium-backend-errors"
        labels:
          severity: critical
          env: prod
          component: backend
          type: availability

      # Backend 5xx Error Rate Spike - Staging
      - uid: backend-5xx-error-rate-stage
        title: "Backend 5xx Error Rate Spike (stage)"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                sum(rate(http_server_request_duration_seconds_count{deployment_environment="stage", http_response_status_code=~"5.."}[5m]))
                / sum(rate(http_server_request_duration_seconds_count{deployment_environment="stage"}[5m]))
                * 100 or vector(0)
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 10  # Higher threshold for staging
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 10m
        annotations:
          summary: "High 5xx error rate in staging backend"
          description: "Backend API 5xx error rate exceeded 10% over the last 10 minutes."
          dashboard_url: "/d/scanium-backend-errors"
        labels:
          severity: warning
          env: stage
          component: backend
          type: availability

      # Backend Sustained High Latency (p95) - Production
      - uid: backend-high-latency-prod
        title: "Backend High Latency (prod)"
        condition: C
        data:
          # Query A: Calculate p95 latency in milliseconds
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                histogram_quantile(0.95,
                  sum by (le) (
                    rate(http_server_request_duration_seconds_bucket{deployment_environment="prod"}[10m])
                  )
                ) * 1000
              queryType: instant
              refId: A
          # Query B: Reduce
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          # Query C: Alert if p95 > 500ms
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 500  # Threshold: p95 > 500ms
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 10m
        annotations:
          summary: "Sustained high latency in production backend"
          description: "Backend API p95 latency exceeded 500ms for over 10 minutes. Performance may be degraded."
          runbook_url: "https://github.com/ilpeppino/scanium/wiki/runbooks/backend-latency"
          dashboard_url: "/d/scanium-backend-api-perf"
        labels:
          severity: warning
          env: prod
          component: backend
          type: latency

      # Backend Sustained High Latency (p95) - Staging
      - uid: backend-high-latency-stage
        title: "Backend High Latency (stage)"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: MIMIR
            model:
              expr: |
                histogram_quantile(0.95,
                  sum by (le) (
                    rate(http_server_request_duration_seconds_bucket{deployment_environment="stage"}[10m])
                  )
                ) * 1000
              queryType: instant
              refId: A
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 1000  # Higher threshold for staging: 1s
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 15m
        annotations:
          summary: "Sustained high latency in staging backend"
          description: "Backend API p95 latency exceeded 1000ms for over 15 minutes."
          dashboard_url: "/d/scanium-backend-api-perf"
        labels:
          severity: info
          env: stage
          component: backend
          type: latency

      # Backend Error Rate Spike (from logs) - Production
      - uid: backend-log-error-spike-prod
        title: "Backend Log Error Spike (prod)"
        condition: C
        data:
          # Query A: Count error logs over 10m
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: LOKI
            model:
              expr: |
                sum(count_over_time({source="scanium-backend", env="prod"} | json | level=~"error|fatal" [10m]))
              queryType: instant
              refId: A
          # Query B: Reduce to single value
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          # Query C: Threshold check (> 100 errors in 10m = alert)
          - refId: C
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 100  # Threshold: > 100 errors/10min
                  operator:
                    type: and
                  reducer:
                    type: last
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          summary: "High error log volume in production backend"
          description: "Backend error logs exceeded 100 in the last 10 minutes. Check logs for details."
          dashboard_url: "/d/scanium-logs-explorer"
        labels:
          severity: warning
          env: prod
          component: backend
          signal: logs